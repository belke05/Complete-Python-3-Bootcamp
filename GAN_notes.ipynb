{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. simple_GAN_notes.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ntaL99qnF6-X",
        "MaZNEWYQNrYL",
        "Qcw63FyFR-yb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntaL99qnF6-X",
        "colab_type": "text"
      },
      "source": [
        "# Cr√©er des handwritten digits avec des GAN \n",
        "\n",
        "\n",
        "[GAN ou Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) \n",
        "\n",
        "<img src=\"https://www.tensorflow.org/tutorials/generative/images/gan1.png\" width=400>\n",
        "\n",
        "\n",
        "During training, the generator progressively becomes better at creating images that look real, while the discriminator becomes better at telling them apart. The process reaches equilibrium when the discriminator can no longer distinguish real images from fakes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O9B7Fc4F4Wm",
        "colab_type": "code",
        "outputId": "2a45628b-880e-4688-9f6b-2ce2087573d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0beta1\n",
        "!pip install pprint \n",
        "!pip install simple_chalk\n",
        "from simple_chalk import chalk, green, red, blue\n",
        "import tensorflow as tf, numpy as np , matplotlib.pyplot as plt, os, pandas as pd\n",
        "from pprint import pprint\n",
        "from IPython import display\n",
        "import time\n",
        "import os \n",
        "import glob"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0beta1 in /usr/local/lib/python3.6/dist-packages (2.0.0b1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0beta1) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0beta1) (1.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0beta1) (1.11.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0beta1) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0beta1) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0beta1) (0.33.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0beta1) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0beta1) (3.10.0)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0beta1) (1.14.0a20190603)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0beta1) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0beta1) (0.8.1)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0beta1) (1.14.0.dev2019060501)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0beta1) (0.1.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0beta1) (0.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0beta1) (1.17.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0beta1) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0beta1) (42.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0beta1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0beta1) (0.16.0)\n",
            "Requirement already satisfied: pprint in /usr/local/lib/python3.6/dist-packages (0.1)\n",
            "Requirement already satisfied: simple_chalk in /usr/local/lib/python3.6/dist-packages (0.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plRXJPYOIoyz",
        "colab_type": "text"
      },
      "source": [
        "* get data [`tf.keras.datasets.mnist`](https://www.tensorflow.org/api_docs/python/tf/keras/datasets)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLAkx8r8RDhx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "2a2d3e52-5391-4ed7-9c29-5875892ae763"
      },
      "source": [
        "mnist_data = tf.keras.datasets.mnist.load_data()\n",
        "x_train, y_train = mnist_data[0]\n",
        "print(red(x_train.shape), red('is the shape for train img'))\n",
        "print(red(y_train.shape), red('is the shape for train labels'))\n",
        "\n",
        "x_test, y_test = mnist_data[1]\n",
        "print(green(x_test.shape), green('is the shape for test img'))\n",
        "print(green(y_test.shape), green('is the shape for test labels'))\n",
        "\n",
        "WIDTH = x_train.shape[-1]\n",
        "HEIGHT = x_train.shape[-2]\n",
        "TEST_SAMPLES = y_test.shape[0]\n",
        "TRAIN_SAMPLES = y_test.shape[0]"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31m(60000, 28, 28)\u001b[0m \u001b[31mis the shape for train img\u001b[0m\n",
            "\u001b[31m(60000,)\u001b[0m \u001b[31mis the shape for train labels\u001b[0m\n",
            "\u001b[32m(10000, 28, 28)\u001b[0m \u001b[32mis the shape for test img\u001b[0m\n",
            "\u001b[32m(10000,)\u001b[0m \u001b[32mis the shape for test labels\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnJEYEbGJK1Q",
        "colab_type": "text"
      },
      "source": [
        "<h3> pre processing </h3>\n",
        "\n",
        "  * reshape images into following form (sample, height, width, channels = 1). \n",
        "  * make images `float32`\n",
        "  * we want pixels between intervals [-1, 1]. Vous pourrez tout simplement soustraire chacun des pixels par 127.5 (255/2) puis diviser le tout par 127.5 (255/2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LTv_q3uUy3B",
        "colab_type": "text"
      },
      "source": [
        "If you reshape it with a -1 in the first dimension what you are doing is saying that you will feed in a 4D batch of images (shaped [batch_size, height, width, color_channels], and you are allowing the batch size to be dynamic (which is common to do)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C52FSdZ_Sxie",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f34b8fe4-81e4-4074-8bea-ac99d01f73fd"
      },
      "source": [
        "X_train = tf.reshape(x_train, [-1, HEIGHT, WIDTH, 1])\n",
        "X_train = tf.cast(X_train, dtype=tf.float32)\n",
        "print(X_train[0][10].numpy(), red('pixel values line 10'))\n",
        "X_train = X_train / 255\n",
        "print(X_train[0][10].numpy(), green('normalized pixel values line 10'))\n",
        "X_train.shape"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [ 14.]\n",
            " [  1.]\n",
            " [154.]\n",
            " [253.]\n",
            " [ 90.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]] \u001b[31mpixel values line 10\u001b[0m\n",
            "[[0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.05490196]\n",
            " [0.00392157]\n",
            " [0.6039216 ]\n",
            " [0.99215686]\n",
            " [0.3529412 ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]] \u001b[32mnormalized pixel values line 10\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([60000, 28, 28, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhl-n_U4Kgqu",
        "colab_type": "text"
      },
      "source": [
        "Random image display"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8uXrGtQXVII",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "e686b32c-ff69-4a1c-d607-58e72d294d71"
      },
      "source": [
        "random_index = np.random.randint(TRAIN_SAMPLES)\n",
        "random_images = X_train[random_index]\n",
        "squeezed_img = tf.squeeze(random_images)\n",
        "plt.axis('off')\n",
        "plt.imshow(squeezed_img)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8c256ccb38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAF9klEQVR4nO3dS4iVdRzG8XPGJi0zslJIULoZRtkF\nqZgksEIIhzaRYqAtCpLM7kW1alXUwi6Y3Va2CEUXGRaEhkHQaDerRRetDIISwsoo6KLOaRW0mPc3\n6FHPY30+Sx/eM4fgyx/68860O51OC8jT1+svAIxMnBBKnBBKnBBKnBDqmGqc2zff/8qFw2zT8Lr2\nSP/u5IRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ\n4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ5Z8AhH/b+fhAuU+b9V25bzx3fbmv\n2HNm4/bSynnls5OfHSr3o5GTE0KJE0KJE0KJE0KJE0KJE0KJE0K55/yf6Rs3rtz3vT6pcftixsry\n2eFWZ5S9dttJXzdu/cs2lM++/ual5b5/R/Nnp3JyQihxQihxQihxQihxQihxQihxQij3nP8z25df\nWO8zni3Wdvnshc/dXu6nr9lV7p/ff2rjdu2sj8tnt9/afD/barVak9+r9xNXby33XnByQihxQihx\nQihxQihxQihxQihXKUeZMaecXO7fLZ5R7usHnxjlJ/Q3Lue/uKx8ctqj75b7/uH95X7Okm8at23X\nX1Y+OzxvX7lP3Liz3Otv1htOTgglTgglTgglTgglTgglTgglTgjV7nSaf53h3L759e865Mi7dGY5\nv/bKqq4+/uVfT2vcVs+Y0tVnM7JNw+tGfBfPyQmhxAmhxAmhxAmhxAmhxAmhxAmhvM95lNlxc/0n\n/Eaza//v5b72ujnVT+/qZ3NgnJwQSpwQSpwQSpwQSpwQSpwQSpwQyj1nmD8HLyn3F65e1dXnz3nr\njnKf/tm2rj6fQ8fJCaHECaHECaHECaHECaHECaHECaHcc4aZ/FD9dySvPO6Pcv+zs7fcJ3zU3fug\nHDlOTgglTgglTgglTgglTgglTgjlKqUH2v3HNm7jxtRXIaO5YPPScp/+1FBXn8+R4+SEUOKEUOKE\nUOKEUOKEUOKEUOKEUO45e2D7iosatw3Tnuvqs0/8wCth/xVOTgglTgglTgglTgglTgglTgglTgjl\nnrMHFlz2XuPW3x7T1Wd/+MAz5T5zwrJyn/qI9z1TODkhlDghlDghlDghlDghlDghlDghlHvOw2DP\njQPl/vCkpxu3vZ3u7jlHM35gd7nvu2pW43bM5g8P9deh4OSEUOKEUOKEUOKEUOKEUOKEUOKEUO45\nD4PhUf6rdvvOZjeGLl5d7rPvXdi4Tdx8qL8NFScnhBInhBInhBInhBInhBInhHKVEubTv/aV+7Lt\nN5T7fWdtLPfB438p93cuWtP87MBN5bPtLZ+UOwfGyQmhxAmhxAmhxAmhxAmhxAmhxAmh3HOGWfXT\n7HIff83Ocr9v+aJyH1y48oC/0z++WjSu3KdvOeiPZgROTgglTgglTgglTgglTgglTgglTgjlnjPM\nklPeLvdbB+8s93Oe/6H+Ac2/+ZIwTk4IJU4IJU4IJU4IJU4IJU4IJU4I5Z4zzNn9Y8v9x/P6y/34\nrXvK/e7vLy/3J6cMlTtHjpMTQokTQokTQokTQokTQokTQrlKOQwmbdld7mt/m9y4LTihfuVrzdLl\n5X7znMXl/uqUTeVeOXPd3oN+lgPn5IRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ7U6n0zjO7ZvfPHLQvnls\noHHbtujJ8tmx7fqVsW79PPxH47Z4av3nCTk4m4bXtUf6dycnhBInhBInhBInhBInhBInhBInhPI+\nZw+c8eCWxu2Kb+8pn1289I1yv33il+Ve3WO2Wq3WglvuatzGtt4vn+XQcnJCKHFCKHFCKHFCKHFC\nKHFCKHFCKO9zQo95nxOOMuKEUOKEUOKEUOKEUOKEUOKEUOKEUOKEUOKEUOKEUOKEUOKEUOKEUOKE\nUOKEUOKEUOKEUOKEUOKEUOKEUOKEUOKEUOKEUOKEUOKEUOKEUOKEUOKEUOKEUOWfAAR6x8kJocQJ\nocQJocQJocQJocQJof4GKxbAAc1/eN8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43IiifiAK3j7",
        "colab_type": "text"
      },
      "source": [
        "[`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n",
        "\n",
        "we will use dataset_x_train in the the end to train our generator "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWFfyoTGKnEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "BUFFER_SIZE = 6000\n",
        "dataset_x_train  = tf.data.Dataset.from_tensor_slices(X_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset_y_train = tf.data.Dataset.from_tensor_slices(y_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset_train = tf.data.Dataset.zip((dataset_x_train, dataset_y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBeOX3vNZJoJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4cfbb19a-c7c0-4f95-fd89-5da4a1f49e35"
      },
      "source": [
        "X_test = tf.reshape(x_test, [-1, HEIGHT, WIDTH, 1])\n",
        "X_test = tf.cast(X_test, dtype=tf.float32)\n",
        "print(X_test[0][10].numpy(), red('pixel values line 10'))\n",
        "X_test = (X_test - 127.5) / 127.5 # we use tang activation func in end so needs to be within -1/1\n",
        "print(X_test[0][10].numpy(), green('normalized pixel values line 10'))\n",
        "X_test.shape"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [ 17.]\n",
            " [ 66.]\n",
            " [ 14.]\n",
            " [ 67.]\n",
            " [ 67.]\n",
            " [ 67.]\n",
            " [ 59.]\n",
            " [ 21.]\n",
            " [236.]\n",
            " [254.]\n",
            " [106.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]\n",
            " [  0.]] \u001b[31mpixel values line 10\u001b[0m\n",
            "[[-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-0.8666667 ]\n",
            " [-0.48235294]\n",
            " [-0.8901961 ]\n",
            " [-0.4745098 ]\n",
            " [-0.4745098 ]\n",
            " [-0.4745098 ]\n",
            " [-0.5372549 ]\n",
            " [-0.8352941 ]\n",
            " [ 0.8509804 ]\n",
            " [ 0.99215686]\n",
            " [-0.16862746]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]] \u001b[32mnormalized pixel values line 10\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([10000, 28, 28, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir-A73VAL9mx",
        "colab_type": "code",
        "outputId": "fd71d4c8-099d-4d53-c1bc-51048b5c574b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "features, labels = next(iter(dataset_train.take(1)))\n",
        "print(features.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 28, 28, 1)\n",
            "(256,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qEHJ74InWpN",
        "colab_type": "text"
      },
      "source": [
        "How it works\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1814/1*Sqhji7Zz4IK2HDgCOabhXQ.png\" width=500>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaZNEWYQNrYL",
        "colab_type": "text"
      },
      "source": [
        "# _generator_. \n",
        "\n",
        "1.  takes noise as input than we increasingly our generated images become better. this is our seed \n",
        "\n",
        "2.  A first dense layer will take this input\n",
        "\n",
        "3.  upscaling until we get the image of size (28,28,1)\n",
        "\n",
        "üö®üö® Notice the tf.keras.layers.LeakyReLU activation for each layer, except the output layer which uses tanh. üö®üö® \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P66bOjuHeCti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NOISE_N = 100\n",
        "IMAGE_DIM = 28\n",
        "\n",
        "noise = tf.random.normal([1,100])\n",
        "\n",
        "# in between we have trainable Weights and Bias\n",
        "\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    # 7w by 7h 256filters \n",
        "    model.add(tf.keras.layers.Dense(7*7*256, use_bias=False, input_shape=(NOISE_N,)))\n",
        "    # avoid overfitting \n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    model.add(tf.keras.layers.Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
        "\n",
        "    # conv is matrix if you multiply this by an img you get output conv \n",
        "    # here we want to do the reverse \n",
        "    model.add(tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    # 2x2 stride because we do inverse of conv\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    # our last upscaling we want to be sure it is the dimensions of the image\n",
        "    assert model.output_shape == (None, IMAGE_DIM, IMAGE_DIM, 1)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV010y8QO8AZ",
        "colab_type": "text"
      },
      "source": [
        "[LeakyRelu()](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LeakyReLU) \n",
        "\n",
        "It allows a small gradient \n",
        "\n",
        "*  when the unit is not active: f(x) = alpha * x for x < 0 --> leaves a gradient instead of putting the neuron at 0 when not active (what relu does)\n",
        "*  f(x) = x for x >= 0.\n",
        "\n",
        "\n",
        " [_Conv2DTranspose_](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2DTranspose)\n",
        "\n",
        "The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqTLjPpvQ58X",
        "colab_type": "text"
      },
      "source": [
        "* random noise image: \n",
        "  * tensor with random pixels ---> [`tf.random.normal`](https://www.tensorflow.org/api_docs/python/tf/random/normal)\n",
        "  * apply the noise to the generator \n",
        "  * Visualisez avec plt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fo3N8ybhpuP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "a2c0df9c-da90-48ce-ad5e-d64aa1253d21"
      },
      "source": [
        "generator = make_generator_model()\n",
        "rndm_img = generator(noise)\n",
        "print(rndm_img.shape)\n",
        "plt.axis('off')\n",
        "plt.imshow(tf.squeeze(rndm_img))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 28, 28, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8c25446e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nk8goFieRUSxOIqNYnERGsTiJjPpfOq+n1vWPHqoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qcw63FyFR-yb",
        "colab_type": "text"
      },
      "source": [
        "# __Discriminator__\n",
        "\n",
        "= will try to predict if our generated image is a true one or not so can be a binary classifier \n",
        "\n",
        "*  Dense layer: A linear operation in which every input is connected to every output by a weight (so there are n_inputs * n_outputs weights - which can be a lot!). Generally followed by a non-linear activation function\n",
        "*  Convolutional layer: A linear operation using a subset of the weights of a dense layer. Nearby inputs are connected to nearby outputs (specifically - a convolution 713 ). The weights for the convolutions at each location are shared. Due to the weight sharing, and the use of a subset of the weights of a dense layer, there‚Äôs far less weights than in a dense layer. Generally followed by a non-linear activation function\n",
        "*  Pooling layer: Replace each patch in the input with a single output, which is the maximum (can also be average) of the input patch\n",
        "*  Normalisation layer: Scale the input so that the output has near to a zero mean and unit standard deviation, to allow for faster and more resilient training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URqEW06aRrfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# output this time is 0/1 --> 0 false 1 true \n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                                     input_shape=[IMAGE_DIM, IMAGE_DIM, 1]))\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    # adds some non linearity \n",
        "    model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    # to go from conv to dense\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4-fQX2GWVTr",
        "colab_type": "text"
      },
      "source": [
        "* test predictions of our discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9vWLz1mmT3M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2130a39e-c08b-4d28-e81c-facfac93e46e"
      },
      "source": [
        "discriminator = make_discriminator_model()\n",
        "decision = discriminator(rndm_img)\n",
        "print(green(decision))"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[[-0.00140939]]\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee1KKAMExMJ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7c72e14f-2797-4cab-b64e-a50da06d5f8e"
      },
      "source": [
        "decision2 = discriminator(np.random.rand(1,28,28,1).astype(\"float32\"))\n",
        "print(decision2)\n",
        "print(red('closer to 0 means it classifies it as fake'))"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.01260547]], shape=(1, 1), dtype=float32)\n",
            "\u001b[31mcloser to 0 means it classifies it as fake\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nP6YsTAWqRj",
        "colab_type": "text"
      },
      "source": [
        "# Loss functions \n",
        "\n",
        "\n",
        "  * **discriminator_loss** \n",
        "  * **generator_loss** \n",
        "  \n",
        "<img src=\"https://i.stack.imgur.com/rNI4P.png\" width=500>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-tB_T12WjD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWqSPu-aX_L2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "b54192a1-2cb5-499b-f210-16b94b721dc0"
      },
      "source": [
        "# real_img_pred = discriminator(real_images)\n",
        "# fake_img_pred = discriminator(real_images)\n",
        "print(red('the predictions look like this'),  decision)\n",
        "print(green('the ones like looks like this'),  tf.ones_like(decision))\n",
        "print(blue('the zero like looks like this'),  tf.zeros_like(decision))\n",
        "print(red('example of cross entropy real'),  cross_entropy(tf.ones_like(decision), decision))\n",
        "print(green('example of cross entropy fake'),  cross_entropy(tf.zeros_like(decision), decision))\n",
        "\n",
        "def discriminator_loss(real_img_predictions, fake_img_predictions):\n",
        "  real_loss = cross_entropy(tf.ones_like(real_img_predictions), real_img_predictions) \n",
        "  # we use ones because the values should be 1 \n",
        "  # the real entropy / loss will be bigger if real_img_predictions is close to 0 which means \n",
        "  # the discrim thinks it is fake\n",
        "  fake_loss = cross_entropy(tf.zeros_like(fake_img_predictions), fake_img_predictions)\n",
        "  # we use zeros because the values should be 0 -- because these are fakes\n",
        "  # the fake entropy / loss will be bigger if fake_img_predictions is close to 1 which means \n",
        "  # the discrim thinks it is real \n",
        "  total_loss = real_loss + fake_loss\n",
        "  return total_loss\n",
        "  # in total we want total loss to be low because \n",
        "  # this means it does well in telling that fake is fake and real is real "
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mthe predictions look like this\u001b[0m tf.Tensor([[-0.00140939]], shape=(1, 1), dtype=float32)\n",
            "\u001b[32mthe ones like looks like this\u001b[0m tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
            "\u001b[34mthe zero like looks like this\u001b[0m tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\u001b[31mexample of cross entropy real\u001b[0m tf.Tensor(0.6938521, shape=(), dtype=float32)\n",
            "\u001b[32mexample of cross entropy fake\u001b[0m tf.Tensor(0.6924427, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUah8h88a2bS",
        "colab_type": "text"
      },
      "source": [
        "* `generator_loss` which finds at which moment do we dupe `discriminator`. \n",
        "\n",
        "*  `generator_loss` will go down if `discriminator` defines fake img as a real one\n",
        "\n",
        "*  if the prediction is closer to 1 this means our generator is doing better / for this our generator should be rewarded \n",
        "\n",
        "*  which also means our cross_entropy / loss will diminish"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFUG81sKat7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_loss(fake_img_predictions):\n",
        "  return cross_entropy(tf.ones_like(fake_img_predictions), fake_img_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjIuU_7PcCd7",
        "colab_type": "text"
      },
      "source": [
        "* D√©finissons maintenant deux optimiseurs pour nos deux fonctions de co√ªt. Cr√©ez deux optimiseurs Adam avec un learning rate √† `1e-4` chacun "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foP-A9yqcBZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.001\n",
        "\n",
        "optimizer_generator = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "optimizer_discriminator = tf.keras.optimizers.Adam(learning_rate=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSXRZr5ocg_1",
        "colab_type": "text"
      },
      "source": [
        "# training training training\n",
        "\n",
        "---\n",
        "\n",
        "start by defining some variables:\n",
        "\n",
        "\n",
        "\n",
        "*  EPOCHS / NOISE_N \n",
        "\n",
        "*  num_examples_to_generate = 16 (random images to make)\n",
        "\n",
        "*  seed = tf.random.normal([num_examples_to_generate, noise_dim]) ----> our random images seed array  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2trFkvbcTX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 50\n",
        "num_examples_to_generate = 16\n",
        "SEED = tf.random.normal([num_examples_to_generate, NOISE_N])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3KH3sw35rky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_apply_gradients(fake_img_predictions ,\n",
        "              real_img_predictions, \n",
        "              generator_model=generator, \n",
        "              discriminator_model=discriminator, \n",
        "              generator_optimizer=optimizer_generator, \n",
        "              discriminator_optimizer=optimizer_discriminator):\n",
        "  \n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    # our loss function is cross entropy\n",
        "    gen_loss = generator_loss(fake_img_predictions) # will be rewarded if discriminator makes mistakes\n",
        "    disc_loss = discriminator_loss(real_img_predictions, fake_img_predictions)\n",
        "    \n",
        "    \n",
        "  gen_gradients = gen_tape.gradient(gen_loss, generator_model.trainable_variables)\n",
        "  disc_gradients = disc_tape.gradient(disc_loss, discriminator_model.trainable_variables)\n",
        "\n",
        "  # we need to apply the found gradients to the variables of the model \n",
        "  generator_optimizer.apply_gradients(zip(gen_gradients, generator_model.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(disc_gradients, discriminator_model.trainable_variables))\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uof7Ybcnd7gc",
        "colab_type": "text"
      },
      "source": [
        "------\n",
        "\n",
        "<h1>Train step</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loDB0kH1d64N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def step_train(imagebatch):\n",
        "  noise=tf.random.normal([BATCH_SIZE, NOISE_N])\n",
        "  \n",
        "  \n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    generated_img = generator(noise)\n",
        "    real_img_predictions = discriminator(imagebatch)\n",
        "    fake_img_predictions = discriminator(generated_img)\n",
        "    # our loss function is cross entropy\n",
        "    gen_loss = generator_loss(fake_img_predictions) # will be rewarded if discriminator makes mistakes\n",
        "    disc_loss = discriminator_loss(real_img_predictions, fake_img_predictions)\n",
        "    \n",
        "    \n",
        "  gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "  disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "  # we need to apply the found gradients to the variables of the model \n",
        "  optimizer_generator.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
        "  optimizer_discriminator.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQuGOqJB-bKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      step_train(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as we go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             SEED)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpA6_QzNer3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B0FRKMVeM9_",
        "colab_type": "code",
        "outputId": "7e8d3a99-a796-40df-fd0c-361565979b72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "train(dataset_x_train, EPOCHS)"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOwAAADnCAYAAAAdFLrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nghBe2ICACkJ4YQMCKgj/D59pkOPe17gwAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 288x288 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-172-c78145347acf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-171-19f8a812ef81>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Save the model every 15 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m15\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Time for epoch {} is {} sec'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'checkpoint' is not defined"
          ]
        }
      ]
    }
  ]
}